#!/bin/bash

# setup.sh - automatic installation and configuration for WSBG Terminal
# Supports macOS and Linux

set -e

echo "=========================================="
echo "   WSBG Terminal - Setup & Installation   "
echo "=========================================="

# 1. Install Ollama
if ! command -v ollama &> /dev/null; then
    echo "[*] Ollama not found. Installing..."
    curl -fsSL https://ollama.com/install.sh | sh
else
    echo "[*] Ollama is already installed."
fi

# Ensure Ollama is running
if ! pgrep -x "ollama" > /dev/null; then
    echo "[*] Starting Ollama server..."
    ollama serve > /dev/null 2>&1 &
    # Wait for it to respond
    echo "    Waiting for Ollama to be ready..."
    count=0
    while ! curl -s http://localhost:11434/api/tags > /dev/null; do
        sleep 1
        count=$((count + 1))
        if [ $count -gt 30 ]; then
            echo "    Error: Ollama failed to start."
            exit 1
        fi
    done
    echo "    Ollama is ready."
fi

# 2. Detect System Resources (RAM)
OS="$(uname -s)"
RAM_GB=0

if [ "$OS" = "Darwin" ]; then
    RAM_BYTES=$(sysctl -n hw.memsize)
    RAM_GB=$((RAM_BYTES / 1024 / 1024 / 1024))
elif [ "$OS" = "Linux" ]; then
    # Helper to get MemTotal in GB
    RAM_KB=$(grep MemTotal /proc/meminfo | awk '{print $2}')
    RAM_GB=$((RAM_KB / 1024 / 1024))
else
    echo "Unsupported OS: $OS"
    exit 1
fi

echo "[*] Detected System Memory: ${RAM_GB} GB"

# 3. Determine Performance Mode (Smart Selection)
MODE="MED"
if [ "$RAM_GB" -lt 8 ]; then
    MODE="LOW"
elif [ "$RAM_GB" -ge 32 ]; then
    MODE="SUPER"
fi

echo "[*] Selected Optimization Mode: $MODE"

# 4. Select Models based on Mode
# Constant models
VISION_MODEL="glm-ocr:latest"
EMBED_MODEL="nomic-embed-text-v2-moe:latest"
FUNCTION_MODEL="functiongemma:latest"

# Variable models
REASONING_MODEL="gemma3:12b"
TRANSLATOR_MODEL="translategemma:12b"

if [ "$MODE" = "LOW" ]; then
    # Low memory: Use smaller quantization/parameter count
    REASONING_MODEL="gemma3:4b"
    TRANSLATOR_MODEL="translategemma:4b"
elif [ "$MODE" = "SUPER" ]; then
    # High memory: Use larger models
    REASONING_MODEL="gemma3:27b"
    # Translator maxes at 12b effectively per requirements
    TRANSLATOR_MODEL="translategemma:12b"
fi

echo "[*] Configuration Roadmap:"
echo "    - Reasoning Agent: $REASONING_MODEL"
echo "    - Translator:      $TRANSLATOR_MODEL"
echo "    - Vision/OCR:      $VISION_MODEL"
echo "    - Embeddings:      $EMBED_MODEL"
echo "    - Functions:       $FUNCTION_MODEL"

# 5. Pull Models
echo "[*] Pulling models via Ollama (this may take a while)..."

pull_model() {
    echo "    > Pulling $1..."
    ollama pull "$1"
}

pull_model "$REASONING_MODEL"
pull_model "$TRANSLATOR_MODEL"
pull_model "$VISION_MODEL"
pull_model "$EMBED_MODEL"
pull_model "$FUNCTION_MODEL"

# 6. Generate Configuration File
echo "[*] Generating Application Configuration..."

# Detech AppData path (Matching StorageUtils logic)
CONFIG_DIR=""
if [ "$OS" = "Darwin" ]; then
    CONFIG_DIR="$HOME/Library/Application Support/wsbg-terminal"
else
    # Linux
    if [ -n "$XDG_DATA_HOME" ]; then
        CONFIG_DIR="$XDG_DATA_HOME/wsbg-terminal"
    else
        CONFIG_DIR="$HOME/.local/share/wsbg-terminal"
    fi
fi

mkdir -p "$CONFIG_DIR"
CONFIG_FILE="$CONFIG_DIR/config.toml"

# Write TOML
cat > "$CONFIG_FILE" <<EOL
# WSBG Terminal Configuration
# Auto-generated by setup.sh based on detected hardware (${RAM_GB} GB RAM)
# Mode: ${MODE}

debug-mode = false
ui-reddit-visible = true

[agent]
ollama.model = "${REASONING_MODEL}"
ollama.translator-model = "${TRANSLATOR_MODEL}"
ollama.vision-model = "${VISION_MODEL}"
ollama.embedding-model = "${EMBED_MODEL}"

[reddit]
# Add reddit settings here if needed
EOL

echo "[*] Configuration written to: $CONFIG_FILE"
echo ""
echo "=========================================="
echo "   Setup Complete! Ready to Run.          "
echo "=========================================="
echo "Run using: .script/run.sh"
